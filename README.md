# Задание 6: CNN для классификации изображений (Fashion-MNIST) 
Введение

В данной работе рассматривается задача классификации изображений одежды (Fashion-MNIST) с помощью сверточной нейронной сети (CNN). Fashion-MNIST – это набор данных из 70 000 градаций серого изображений размера 28×28 пикселей, разделённых на 10 классов (футболки, брюки, платья и др.). В отличие от классического MNIST, здесь изображения сложнее, поэтому стандартная архитектура CNN имеет несколько сверточных и полносвязных слоёв, а для повышения обобщающей способности применяют аугментацию данных и регуляризацию (Dropout).

Основная часть
Архитектура сети

Сеть строится по заданной методичкой схеме:

Conv2D(32, 3×3, padding='same', activation='relu') → MaxPooling2D(2×2) – первый сверточный слой с 32 фильтрами и функция активации ReLU;

Conv2D(64, 3×3, padding='same', activation='relu') → MaxPooling2D(2×2) – второй сверточный слой с 64 фильтрами и ReLU;

Flatten() – преобразование объёма признаков в вектор;

Dense(128, activation='relu') → Dropout(0.5) – полносвязный слой на 128 нейронов с ReLU и слой Dropout для регуляризации (вероятность отключения 50%);

Dense(10, activation='softmax') – выходной слой на 10 нейронов с softmax-активацией для многоклассовой классификации.

Использование ReLU в скрытых слоях ускоряет обучение и помогает избежать проблемы затухающих градиентов (в отличие от сигмоидных активаций). Dropout снижает переобучение за счёт «выключения» части нейронов во время обучения, что улучшает обобщающую способность модели. Выходной слой с softmax даёт вектор вероятностей по классам, что соответствует использованию многоклассовой функции потерь (categorical crossentropy).

Предобработка и аугментация данных

Данные загружаются из keras.datasets.fashion_mnist и нормализуются: значения пикселей приводятся к диапазону [0,1] (делением на 255). Набор состоит из 60 000 тренировочных и 10 000 тестовых изображений. Для подачи в Conv2D к изображениям добавляется канал: форма данных меняется с (28,28) на (28,28,1).

Для уменьшения переобучения применяется аугментация: в качестве трансформаций используются случайный поворот изображения (до ±10%), масштаб (zoom) и сдвиг (translation). В сумме эти преобразования искусственно увеличивают разнообразие обучающего множества, улучшая обобщение модели (как отмечено в литературе, data augmentation широко используется для повышения обобщающей способности сетей). Аугментация реализуется как последовательность слоёв RandomRotation, RandomZoom и RandomTranslation.

Компиляция и обучение

Сеть компилируется с оптимизатором Adam (быстро сходится на задаче классификации изображений) и функцией потерь sparse_categorical_crossentropy (поскольку метки целых чисел). В качестве метрики используется точность (accuracy).

Обучение проводится итеративно. Набор разбивается на обучающую и валидационную часть (validation_split=0.2), для наилучшей настройки гиперпараметров, а данные проходят через аугментацию на лету. Модель обучается несколько эпох с небольшим размером батча (например, 32–64). Dropout и аугментация предотвращают переобучение. В результате обучение сходится к точности порядка ~0.9 на тесте (в известных работах CNN на Fashion-MNIST достигают ~88–95% точности в зависимости от архитектуры и аугментации).

Результаты

После обучения код выводит значение точности на тестовой выборке. Например, на стандартной архитектуре без донастройки может получиться точность ~0.88–0.90. (В приведённом коде в конце выводится строка print(f"Test accuracy: {test_acc:.4f}").) Визуализация сгенерированных фильтров или ошибок классификации при желании могла бы показаться в дополнительном анализе.

# Ответы на контрольные вопросы

3. Что такое теорема Ардена и для чего она используется? Теорема (или правило) Ардена – это утверждение из теории автоматов, описывающее решение уравнения вида $R = Q + RP$, где $R,Q,P$ – регулярные выражения, а $P$ не содержит пустого слова. Согласно Ардену, его единственное решение – $R = QP^*$. Фактически она позволяет выразить регулярное выражение через итерацию над шаблоном. Практическое применение – превращение конечного автомата в эквивалентное регулярное выражение (например, в алгоритме Клини).

6. Почему алгоритмы во внешней памяти измеряют сложность в операциях I/O, а не в сравнениях? Во внешней памяти (out-of-core) данные хранятся на внешних носителях (диски) и не помещаются целиком в ОЗУ. Основной тормоз – это стоимость операций чтения/записи блоков с диска. В модели внешней памяти время работы алгоритма определяется числом операций ввода-вывода (перемещением блоков между памятью и диском). Сравнения или арифметические операции в памяти относительно быстры по сравнению с медленным доступом к диску, поэтому для алгоритмов во внешней памяти принято учитывать сложность именно в блоках I/O.

9. Что такое диаграммы Вороного и где они применяются? Диаграмма Вороного – это разбиение пространства на ячейки по ближайшему признаку. Формально, для множества точек (генераторов) каждая точечная клетка содержит все точки плоскости, которые ближе к данной исходной точке, чем к любой другой. Ячейки, называемые многоугольниками Вороного, тесселируют пространство. Практически диаграммы Вороного используются для моделирования зон влияния вокруг объектов (например, деление территорий по ближайшему городу), а также в компьютерной графике, геоинформационных системах, робототехнике и других областях, где важно быстро найти ближайший объект.

12. Объясните принцип работы алгоритма Fuzzy C-Means. Чем он отличается от обычного K-Means? Алгоритм Fuzzy C-Means – это «мягкий» (нечеткий) вариант кластеризации. В нём каждая точка данных получает не одно «жёсткое» присвоение кластеру, а набор коэффициентов принадлежности (от 0 до 1) ко всем кластерам. Итеративно обновляются центры кластеров и эти коэффициенты: центр вычисляется как средневзвешенное точек по их степеням принадлежности, а принадлежность точки к кластеру обратно пропорциональна её расстоянию до этого центра (с учётом параметра «жесткости» $m$). В противоположность этому, алгоритм k-Means – это жёсткая кластеризация: каждая точка относится ровно к одному кластеру. Таким образом, главное отличие – в Fuzzy C-Means точки могут частично принадлежать сразу к нескольким кластерам, тогда как k-Means указывает лишь один ближайший кластер.
